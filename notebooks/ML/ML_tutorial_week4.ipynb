{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network and TensorFlow\n",
    "[Artificial Neural Network](https://en.wikipedia.org/wiki/Artificial_neural_network) (ANNs) are a family of models inspired by biological neural networks which are used to estimate or approximate functions that can depend on a large number of inputs and are generally unknown.  \n",
    "\n",
    "### Biological Neuron\n",
    "* There are about one hundred billion neurons in human brain.\n",
    "* Brain is an extremeley interconnected networks of neurons. \n",
    "* A neuron collects inputs using a structure called dendrites, the neuron effectively sums all of these inputs from the dendrites and if the resulting value is greater than it's firing threshold, the neuron fires.\n",
    "* When the neuron fires it sends an electrical impulse through the neuron's axon to it's boutons. These boutons can then be networked to thousands of other neurons via connections called synapses.\n",
    "<img src=\"images/biological_neuron.png\" width=50%>  \n",
    "\n",
    "### Perceptron (or Artificial Neuron)\n",
    "A typical perceptron will have many inputs and these inputs are all individually weighted. The perceptron weights can either amplify or deamplify the original input signal. For example, if the input is 1 and the input's weight is 0.2 the input will be decreased to 0.2. These weighted signals are then added together and passed into the activation function. The activation function is used to convert the input into a more useful output. There are many different types of activation function but one of the simplest would be step function. A step function will typically output a 1 if the input is higher than a certain threshold, otherwise it's output will be 0.  \n",
    "\n",
    "**Two step process**\n",
    "1. Calculate the weighted sum\n",
    "2. Use the activation function to decide the output of of the neuron\n",
    "<img src=\"images/perceptron.png\" width=50%>\n",
    "\n",
    "**Types of activation functions**\n",
    "1. [Step function](https://en.wikipedia.org/wiki/Step_function)  \n",
    "2. [ReLU (Rectified Linear units)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))\n",
    "3. [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) \n",
    "4. [tanh](https://en.wikipedia.org/wiki/Hyperbolic_function)\n",
    "5. [Softmax](https://en.wikipedia.org/wiki/Softmax_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def step_activation_function(val, threshold):\n",
    "    if val>threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def weighted_sum(inp, wt):\n",
    "    temp_sum = 0\n",
    "    for i in range(len(inp)):\n",
    "        temp_sum += inp[i]*wt[i]\n",
    "    return temp_sum\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### OR Gate\n",
    "# Input_1 Input_2 Output\n",
    "#    0       0      0\n",
    "#    0       1      1\n",
    "#    1       0      1\n",
    "#    1       1      1\n",
    "\n",
    "inp = [0, 0]\n",
    "wt = [2.0, 2.0]\n",
    "threshold = 1.0\n",
    "\n",
    "## Step 1: Calculate the weighted sum\n",
    "wt_sum = weighted_sum(inp, wt)\n",
    "\n",
    "## Step 2: Use the activation function to decide the output of of the neuron\n",
    "out = step_activation_function(wt_sum, threshold)\n",
    "print out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR gate problem\n",
    "Life was simple back then! Things were going good with single layer perceptron. Boats were sailing, computers were happily running in one big giant hall! Until one day, two researchers: Marvin Minsky and Seymour Papert published a paper that demonstrated two major problems with single layer perceptron: \n",
    "* Single layer perceptron could not learn xor gate (or any data that is not linearly separable)\n",
    "* The \"tiny GIANT computers\" back then didn't have enough processing power to effectively handle the long run time required by large neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "<img src=\"images/neural_network.png\" width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed forward Neural Network\n",
    "A [feedforward neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network) is an artificial neural network wherein connections between the units do not form a cycle.  \n",
    "* Input Layer: Nodes of the network that accept input values. In the network below, **nodes 1, 2 and 3** are **input nodes**. They do not compute anything, but simply pass the values to the processing nodes in the next layers.\n",
    "* Ouput Layer: These nodes provide us with the output. **Nodes (6 and 7)** are associated with the **output variables**.\n",
    "* Hidden Layer: If neural network was a black box then thesea re the layers that are not visible. They compute values depending on the weight values between interconnections. **Nodes (4 and 5)** are **hidden nodes**.\n",
    "\n",
    "<img src=\"images/feed_forward.png\" width=40%>\n",
    "\n",
    "#### Exercise:\n",
    "<img src=\"images/feed_forward_example.png\" width=45% align=\"middle\">\n",
    "\n",
    "### Training the network (Learning the weights)  \n",
    "1. Randomly initialize all the weights ($w_{ij}$)\n",
    "2. Repeat:  \n",
    "  * Feed the network with an input x from one of the examples in the training set  \n",
    "  * Compute the networkâ€™s output f(x)  \n",
    "  * Change the weights $w_{ij}$ of the nodes  \n",
    "3. Until the error is small\n",
    "\n",
    "### Back propagation algorithm (How to update the weights)\n",
    "<img src=\"images/backpropagation.png\" width=50%>\n",
    "#### Further readings on backpropagation\n",
    "[Derivation of backpropagation rule](https://www.cs.swarthmore.edu/~meeden/cs81/s10/BackPropDeriv.pdf)  \n",
    "[Detailed step by step backpropagation explained](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A  B  C    Output\n",
    "# 0  0  0      0\n",
    "# 0  0  1      0\n",
    "# 0  1  0      0\n",
    "# 0  1  1      1\n",
    "# 1  0  0      0\n",
    "# 1  0  1      1\n",
    "# 1  1  0      0\n",
    "# 1  1  1      1\n",
    "\n",
    "def my_func(a,b,c):\n",
    "    return (a and b) and c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tB\tC\tOutput\n",
      "0\t0\t0\t   0\n",
      "\n",
      "0\t0\t1\t   0\n",
      "\n",
      "0\t1\t0\t   0\n",
      "\n",
      "0\t1\t1\t   0\n",
      "\n",
      "1\t0\t0\t   0\n",
      "\n",
      "1\t0\t1\t   0\n",
      "\n",
      "1\t1\t0\t   0\n",
      "\n",
      "1\t1\t1\t   1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_func():\n",
    "    a = [0,0,0,0,1,1,1,1]\n",
    "    b = [0,0,1,1,0,0,1,1]\n",
    "    c = [0,1,0,1,0,1,0,1]\n",
    "    output = map(my_func,a,b,c)\n",
    "    print 'A\\tB\\tC\\tOutput'\n",
    "    for i in range(8):\n",
    "        print '{}\\t{}\\t{}\\t   {}\\n'.format(a[i],b[i],c[i],output[i])\n",
    "        \n",
    "test_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer weight matrix: \n",
      "[[ 0.68938866  0.41013025  0.8326322   0.7337637   0.73493841]\n",
      " [ 0.22237737  0.69619532  0.32822367  0.43028474  0.97347342]\n",
      " [ 0.67292255  0.99456776  0.36244686  0.15701161  0.9343647 ]]\n",
      "\n",
      "\n",
      "Output Layer weight matrix: \n",
      "[[ 0.24766   ]\n",
      " [ 0.5873699 ]\n",
      " [ 0.62776782]\n",
      " [ 0.58351094]\n",
      " [ 0.79018404]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Initialize parameters\n",
    "learning_rate = 10\n",
    "training_iter = 10000\n",
    "num_input = 3\n",
    "num_hidden = 5\n",
    "num_output = 1\n",
    "\n",
    "hidden_layer_wt_matrix = np.random.random([num_input, num_hidden])\n",
    "output_layer_wt_matrix = np.random.random([num_hidden, num_output])\n",
    "print \"Hidden Layer weight matrix: \"\n",
    "print hidden_layer_wt_matrix\n",
    "print \"\\n\"\n",
    "print \"Output Layer weight matrix: \"\n",
    "print output_layer_wt_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_activation_function(val):\n",
    "    return 1.0/(1.0+math.exp(-val))\n",
    "\n",
    "\n",
    "def get_output(inp,PRINT_FLAG=False):\n",
    "    if PRINT_FLAG:\n",
    "        print \"HIDDEN LAYER:\\n\\nStep 1) Weighted Sum:\"\n",
    "        print \"Input Vector:{}\\nHidden Layer Weight matrix:\\n{}\\nWeighted Sum:\\n{}\\n\".format(inp, hidden_layer_wt_matrix, np.dot(inp,hidden_layer_wt_matrix))\n",
    "\n",
    "    hidden_layer_output = map(sigmoid_activation_function, np.dot(inp,hidden_layer_wt_matrix))\n",
    "    if PRINT_FLAG:\n",
    "        print \"Step 2) Activated Value:\"\n",
    "        print hidden_layer_output\n",
    "        print \"\\n\\nOUTPUT LAYER:\\n\\nStep 1) Weighted Sum:\"\n",
    "        print \"Hidden Layer Output:{}\\nOutput Layer Weight matrix:\\n{}\\nWeighted Sum:\\n{}\\n\".format(hidden_layer_output, output_layer_wt_matrix, np.dot(hidden_layer_output, output_layer_wt_matrix))\n",
    "        \n",
    "    final_output = map(sigmoid_activation_function, np.dot(hidden_layer_output, output_layer_wt_matrix))\n",
    "    if PRINT_FLAG:\n",
    "        print \"Step 2) Activated Value:\"\n",
    "        print final_output\n",
    "    return final_output, hidden_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN LAYER:\n",
      "\n",
      "Step 1) Weighted Sum:\n",
      "Input Vector:[0, 0, 0]\n",
      "Hidden Layer Weight matrix:\n",
      "[[ 0.68938866  0.41013025  0.8326322   0.7337637   0.73493841]\n",
      " [ 0.22237737  0.69619532  0.32822367  0.43028474  0.97347342]\n",
      " [ 0.67292255  0.99456776  0.36244686  0.15701161  0.9343647 ]]\n",
      "Weighted Sum:\n",
      "[ 0.  0.  0.  0.  0.]\n",
      "\n",
      "Step 2) Activated Value:\n",
      "[0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "\n",
      "\n",
      "OUTPUT LAYER:\n",
      "\n",
      "Step 1) Weighted Sum:\n",
      "Hidden Layer Output:[0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "Output Layer Weight matrix:\n",
      "[[ 0.24766   ]\n",
      " [ 0.5873699 ]\n",
      " [ 0.62776782]\n",
      " [ 0.58351094]\n",
      " [ 0.79018404]]\n",
      "Weighted Sum:\n",
      "[ 1.41824635]\n",
      "\n",
      "Step 2) Activated Value:\n",
      "[0.8050633526002793]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "inp_mat = [[0,0,0], [0,0,1], [0,1,0], [0,1,1], [1,0,0], [1,0,1], [1,1,0], [1,1,1]]\n",
    "inp = inp_mat[0]\n",
    "\n",
    "out, hidden_layer_out = get_output(inp, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "0\t0\t0\t   0\t\t    0.8050633526\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "0\t0\t1\t   0\t\t    0.863521235447\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "0\t1\t0\t   0\t\t    0.860752749456\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "0\t1\t1\t   0\t\t    0.895953740718\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "1\t0\t0\t   0\t\t    0.868045985097\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "1\t0\t1\t   0\t\t    0.902854930339\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "1\t1\t0\t   0\t\t    0.901121881192\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "1\t1\t1\t   1\t\t    0.92001604348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp in inp_mat:\n",
    "    out,_ = get_output(inp)\n",
    "    print 'A\\tB\\tC\\tTrue Output\\tNeural Network Output\\n'\n",
    "    print '{}\\t{}\\t{}\\t   {}\\t\\t    {}\\n'.format(inp[0],inp[1],inp[2],my_func(inp[0], inp[1], inp[2]),out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Output Layer Weight matrix:\n",
      "[[ 0.24766   ]\n",
      " [ 0.5873699 ]\n",
      " [ 0.62776782]\n",
      " [ 0.58351094]\n",
      " [ 0.79018404]]\n",
      "\n",
      "Updated Output Layer Weight matrix:\n",
      "[[ 0.29650385]\n",
      " [ 0.63981127]\n",
      " [ 0.67609119]\n",
      " [ 0.62997051]\n",
      " [ 0.84513123]]\n",
      "\n",
      "Previous Hidden Layer Weight matrix:\n",
      "[[ 0.68938866  0.41013025  0.8326322   0.7337637   0.73493841]\n",
      " [ 0.22237737  0.69619532  0.32822367  0.43028474  0.97347342]\n",
      " [ 0.67292255  0.99456776  0.36244686  0.15701161  0.9343647 ]]\n",
      "\n",
      "Updated Hidden Layer Weight matrix:\n",
      "[[ 0.6914467   0.41348803  0.83806158  0.73947414  0.73782294]\n",
      " [ 0.22443541  0.6995531   0.33365305  0.43599518  0.97635795]\n",
      " [ 0.67498059  0.99792554  0.36787624  0.16272205  0.93724922]]\n",
      "Iteration 0:0.692308144188\n",
      "Iteration 1000:4.96703283275e-05\n",
      "Iteration 2000:2.18736507233e-05\n",
      "Iteration 3000:1.40996550308e-05\n",
      "Iteration 4000:1.04109120613e-05\n",
      "Iteration 5000:8.24703063752e-06\n",
      "Iteration 6000:6.82189388472e-06\n",
      "Iteration 7000:5.81178834825e-06\n",
      "Iteration 8000:5.05842208785e-06\n",
      "Iteration 9000:4.47504570636e-06\n"
     ]
    }
   ],
   "source": [
    "from operator import mul\n",
    "\n",
    "def test_NN():\n",
    "    MSE = 0.\n",
    "    for inp in inp_mat:\n",
    "        out,_ = get_output(inp)\n",
    "        MSE += math.pow((my_func(inp[0], inp[1], inp[2]) - out[0]), 2)\n",
    "    return MSE/8\n",
    "  \n",
    "def update_weights(inp, hidden_layer_out, out, PRINT_FLAG=False):\n",
    "    delta_wt_output = -1*(my_func(inp[0], inp[1], inp[2]) - out[0])*out[0]*(1-out[0])\n",
    "    \n",
    "    global output_layer_wt_matrix\n",
    "    global hidden_layer_wt_matrix\n",
    "    \n",
    "    new_output_layer_wt_matrix = output_layer_wt_matrix - learning_rate*delta_wt_output*np.reshape(hidden_layer_out,(num_hidden,num_output))\n",
    "    if PRINT_FLAG:\n",
    "        print \"Previous Output Layer Weight matrix:\"\n",
    "        print output_layer_wt_matrix\n",
    "        print \"\\nUpdated Output Layer Weight matrix:\"\n",
    "        print new_output_layer_wt_matrix\n",
    "    \n",
    "    delta_wt_hidden = delta_wt_output*np.multiply(hidden_layer_out, (np.ones((num_output,num_hidden)) - hidden_layer_out))\n",
    "    \n",
    "    if PRINT_FLAG:\n",
    "        print \"\\nPrevious Hidden Layer Weight matrix:\"\n",
    "        print hidden_layer_wt_matrix\n",
    "    \n",
    "    hidden_layer_wt_matrix = hidden_layer_wt_matrix - learning_rate*np.dot(np.reshape(inp,(num_input,1)), delta_wt_hidden*np.reshape(output_layer_wt_matrix,(1,num_hidden)))\n",
    "    \n",
    "    if PRINT_FLAG:\n",
    "        print \"\\nUpdated Hidden Layer Weight matrix:\"\n",
    "        print hidden_layer_wt_matrix\n",
    "    \n",
    "    output_layer_wt_matrix = new_output_layer_wt_matrix\n",
    "\n",
    "out, hidden_layer_out = get_output(inp)\n",
    "update_weights(inp, hidden_layer_out, out, True)\n",
    "\n",
    "count = 0\n",
    "while count!=training_iter:\n",
    "    if count%1000==0:\n",
    "        print \"Iteration {}:{}\".format(count, test_NN())\n",
    "    count += 1\n",
    "    \n",
    "    for inp in inp_mat:\n",
    "        out, hidden_layer_out = get_output(inp)\n",
    "        update_weights(inp, hidden_layer_out, out)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "0\t0\t0\t   0\t\t    1.52243777873e-05\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "0\t0\t1\t   0\t\t    0.000305064197457\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "0\t1\t0\t   0\t\t    0.00109247771051\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "0\t1\t1\t   0\t\t    0.00180655004367\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "1\t0\t0\t   0\t\t    0.000841546375778\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "1\t0\t1\t   0\t\t    0.00215241282623\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "1\t1\t0\t   0\t\t    0.00186431269409\n",
      "\n",
      "A\tB\tC\tTrue Output\tNeural Network Output\n",
      "\n",
      "1\t1\t1\t   1\t\t    0.995674098238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp in inp_mat:\n",
    "    out,_ = get_output(inp)\n",
    "    print 'A\\tB\\tC\\tTrue Output\\tNeural Network Output\\n'\n",
    "    print '{}\\t{}\\t{}\\t   {}\\t\\t    {}\\n'.format(inp[0],inp[1],inp[2],my_func(inp[0], inp[1], inp[2]),out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction to TensorFlow\n",
    "TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) that flow between them.  \n",
    "### Overview\n",
    "* Need to imagine computations as graphs\n",
    "* Nodes in the graph are operations\n",
    "* Edges are data. Data in tensorflow are called Tensors.\n",
    "* Example: Tensor representing a batch of colored images will be of dimensions [batch size, height, width, color channel(RBG)]\n",
    "* To start computation, the graph must be launched in a Session\n",
    "* Session places the graph ops onto Devices, such as CPUs or GPUs\n",
    "* Methods return output in the form of numpy.ndarray in case of Python\n",
    "\n",
    "### Building the graph\n",
    "* Start with constant values because they don't need any input\n",
    "* Pass the output of this constant op to other op nodes that do computation\n",
    "* TensorFlow provides us with a default graph\n",
    "* While adding ops you need to specify the graph to which the node is being added or else it will be added to the default graph. But since we generally work with single graphs this won't be required much\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Example of constant op\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "print hello\n",
    "sess = tf.Session()\n",
    "print hello\n",
    "print sess.run(hello)\n",
    "print tf.shape(hello)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val = [tf.constant([1.]), tf.constant([2.]), tf.constant([3.])]\n",
    "sum_arr = tf.add_n(val)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(sum_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#matrix1 = tf.Variable([[3., 3.]])\n",
    "#matrix2 = tf.Variable([[2.],[2.]])\n",
    "matrix1 = tf.Variable(np.matrix('1. 2.; 3. 4.'))\n",
    "matrix2 = tf.Variable(np.matrix('1. 2.; 3. 4.'))\n",
    "matrix3 = tf.Variable(np.matrix('1. 2.; 3. 4.'))\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "#sess.run(matrix1)\n",
    "#sess.run(matrix2)\n",
    "matrix1.initializer.run(session=sess)\n",
    "matrix2.initializer.run(session=sess)\n",
    "matrix3.initializer.run(session=sess)\n",
    "\n",
    "print sess.run(product)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['out']), biases['out'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 150.718099394\n",
      "Epoch: 0002 cost= 38.228291161\n",
      "Epoch: 0003 cost= 24.256238728\n",
      "Epoch: 0004 cost= 17.059106627\n",
      "Epoch: 0005 cost= 12.234957079\n",
      "Epoch: 0006 cost= 9.245516853\n",
      "Epoch: 0007 cost= 6.869946676\n",
      "Epoch: 0008 cost= 5.025475971\n",
      "Epoch: 0009 cost= 3.843316119\n",
      "Epoch: 0010 cost= 2.864213248\n",
      "Epoch: 0011 cost= 2.147154921\n",
      "Epoch: 0012 cost= 1.596366819\n",
      "Epoch: 0013 cost= 1.206648112\n",
      "Epoch: 0014 cost= 1.022861474\n",
      "Epoch: 0015 cost= 0.764928835\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9446\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost)\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlowDNNClassifier class is deprecated. Please consider using DNNClassifier as an alternative.\n",
      "WARNING:tensorflow:Change warning: `feature_columns` will be required after 2016-08-01.\n",
      "Instructions for updating:\n",
      "Pass `tf.contrib.learn.infer_real_valued_columns_from_input(x)` or `tf.contrib.learn.infer_real_valued_columns_from_input_fn(input_fn)` as `feature_columns`, where `x` or `input_fn` is your argument to `fit`, `evaluate`, or `predict`.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/px/hb0kcbv93nn3t07lv4wtmmw00000gp/T/tmpEOyAHA\n",
      "WARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(784)]), is_sparse=False)\n",
      "WARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.905600\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "from sklearn import metrics\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)\n",
    "\n",
    "classifier = skflow.TensorFlowDNNClassifier(hidden_units=[256, 256], n_classes=10)\n",
    "classifier.fit(mnist.train.images, mnist.train.labels.astype(int))\n",
    "score = metrics.accuracy_score(mnist.test.labels.astype(int), classifier.predict(mnist.test.images))\n",
    "print(\"Accuracy: %f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Recurrent Neural Networks\n",
    "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior. Unlike feedforward neural networks, RNNs can use their internal memory to process arbitrary sequences of inputs. This makes them applicable to tasks such as unsegmented connected handwriting recognition or speech recognition.  \n",
    "**Analogy to Human Brain**: Try speaking out loud from A to Z. When you are done, try it in reverse i.e Z to A. You will find that A to Z is a lot easier then in reverse order. This is because our entire life we have been learning A to Z and not in reverse order. So brain does some optimization on it's own and remembers the sequence. If I ask you what is fifth letter from D in correct order. It is tough to tell right away! \n",
    "\n",
    "#### Vanishing gradient problem and Exploding gradient problem\n",
    "The **vanishing gradient problem** is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the gradient of the error function with respect to the current weight in each iteration of training. Traditional activation functions such as the hyperbolic tangent function have gradients in the range (âˆ’1, 1) or [0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the \"front\" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n and the front layers train very slowly.  \n",
    "When activation functions are used whose derivatives can take on larger values, one risks encountering the related **exploding gradient problem**.\n",
    "\n",
    "##### [LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, n_steps, x)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.add(tf.matmul(outputs[-1], weights['out']), biases['out'])\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images.reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels\n",
    "    #test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    #test_label = mnist.test.labels[:test_len]\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN implementation with skflow (Need to update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.learn as skflow\n",
    "\n",
    "def mnist_rnn_input_op_fn(x):\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, 28])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, 28, x)\n",
    "    return x\n",
    "\n",
    "classifier = skflow.TensorFlowRNNClassifier(rnn_size=28, cell_type='rnn', n_classes=10, input_op_fn=mnist_rnn_input_op_fn)\n",
    "\n",
    "#classifier = skflow.TensorFlowRNNClassifier(rnn_size=28, \n",
    "#    n_classes=10, cell_type='rnn', input_op_fn=mnist_rnn_input_op_fn,\n",
    "#    num_layers=1, bidirectional=False, sequence_length=None,\n",
    "#    steps=1000, optimizer='Adam', learning_rate=0.01, continue_training=True)\n",
    "print(mnist.train.images.shape)\n",
    "print(mnist.train.labels.shape)\n",
    "train_data = mnist.train.images.reshape((55000, 28, 28))\n",
    "classifier.fit(train_data, mnist.train.labels)\n",
    "print mnist.test.images.shape\n",
    "pred = classifier.predict(mnist.test.images.reshape((10000, 28, 28)))\n",
    "score = metrics.accuracy_score(mnist.test.labels.astype(int), pred)\n",
    "print score\n",
    "#print mnist.test.labels[0]\n",
    "#correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(mnist.test.labels, 1))\n",
    "#print correct_prediction\n",
    "#correct_prediction = tf.equal(tf.argmax(classifier.predict(mnist.test.images).reshape(10000, 28, 28), 1), tf.argmax(mnist.test.labels, 1))\n",
    "    # Calculate accuracy\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "#print \"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Neural Networks"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
