{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "def func_predict(x):\n",
    "    return 2*x + 5\n",
    "\n",
    "inp = np.random.uniform(-10,10,10)\n",
    "out = map(func_predict,inp)\n",
    "\n",
    "lin_reg_model = linear_model.LinearRegression()\n",
    "lin_reg_model.fit(inp.reshape(-1,1), out)\n",
    "\n",
    "print lin_reg_model.coef_\n",
    "print lin_reg_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(inp,out,'o')\n",
    "plt.plot(inp,lin_reg_model.predict(inp.reshape(-1,1)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def func_predict_with_error(x):\n",
    "    return 2*x + 5 + np.random.normal(scale=1,size=1)\n",
    "\n",
    "inp_error = np.random.uniform(-10,10,10)\n",
    "out_error = map(func_predict_with_error,inp_error)\n",
    "\n",
    "lin_reg_model_error = linear_model.LinearRegression()\n",
    "lin_reg_model_error.fit(inp_error.reshape(-1,1), out_error)\n",
    "\n",
    "print lin_reg_model_error.coef_\n",
    "print lin_reg_model_error.intercept_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(inp_error, out_error, 'o')\n",
    "plt.plot(inp_error, lin_reg_model_error.predict(inp_error.reshape(-1,1)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance Tradeoff\n",
    "[Illustration for Bias Variable Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit and Underfit\n",
    "A model is said to **overfit** if it does not **generalizes** well for unknown data. The model overfits on training data but does not fit on unknown data.  \n",
    "**Underfitting** refers to a model that can neither model the training data not generalize to new data.  \n",
    "[Illustration of underfit and overfit](http://blog.fliptop.com/blog/2015/03/02/bias-variance-and-overfitting-machine-learning-overview/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Illustration of overfit and underfit through Non-linear regression\n",
    "\n",
    "[Evaluation metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)  \n",
    "[Confusion matrix](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "X_nonlinear_train = np.random.random(size=(20, 1))\n",
    "Y_nonlinear_train = 2 * X_nonlinear_train.squeeze() + 5 + 0.1*np.random.random(20)\n",
    "\n",
    "## We will use 20 points as training data and the rest 10 points as test data\n",
    "X_nonlinear_test = np.ones((10,1)) + np.random.random(size=(10,1))\n",
    "Y_nonlinear_test = 2 * X_nonlinear_test.squeeze() + 5 + 0.1*np.random.random(10)\n",
    "plt.plot(X_nonlinear_train, Y_nonlinear_train,'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lin_model = linear_model.LinearRegression()\n",
    "lin_model.fit(X_nonlinear_train, Y_nonlinear_train)\n",
    "\n",
    "print \"Mean absolute training error for linear model: \", metrics.mean_absolute_error(Y_nonlinear_train, lin_model.predict(X_nonlinear_train))\n",
    "print \"Mean absolute test error for linear model: \", metrics.mean_absolute_error(Y_nonlinear_test, lin_model.predict(X_nonlinear_test))\n",
    "plt.plot(X_nonlinear_train, Y_nonlinear_train,'o')\n",
    "plt.plot(X_nonlinear_train, lin_model.predict(X_nonlinear_train));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Another way is numpy.polyfit\n",
    "from numpy import linspace\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def polynomial_regression(X_train, Y_train, X_test, Y_test, deg=2):\n",
    "    poly = PolynomialFeatures(degree=deg)\n",
    "    X_poly_train = poly.fit_transform(X_train)\n",
    "    X_poly_test = poly.fit_transform(X_test)\n",
    "    poly_model = linear_model.LinearRegression()\n",
    "    poly_model.fit(X_poly_train, Y_train)\n",
    "    \n",
    "    x_sample = linspace(0, 1, 200)\n",
    "    y_sample = poly_model.intercept_\n",
    "    \n",
    "    for i in range(deg+1):\n",
    "        y_sample +=  poly_model.coef_[i]*x_sample**i\n",
    "    \n",
    "    print \"Mean absolute training error for degree={} is: {}\".format(deg, metrics.mean_absolute_error(Y_train, poly_model.predict(X_poly_train)))\n",
    "    print \"Mean absolute validation error for degree={} is: {}\".format(deg, metrics.mean_absolute_error(Y_test, poly_model.predict(X_poly_test)))\n",
    "    \n",
    "    plt.plot(X_train, Y_train, 'o')\n",
    "    plt.plot(x_sample, y_sample);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polynomial_regression(X_nonlinear_train, Y_nonlinear_train, X_nonlinear_test, Y_nonlinear_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polynomial_regression(X_nonlinear, Y_nonlinear, X_nonlinear_test, Y_nonlinear_test, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polynomial_regression(X_nonlinear, Y_nonlinear, X_nonlinear_test, Y_nonlinear_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polynomial_regression(X_nonlinear, Y_nonlinear, X_nonlinear_test, Y_nonlinear_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polynomial_regression(X_nonlinear, Y_nonlinear, X_nonlinear_test, Y_nonlinear_test, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "polynomial_regression(X_nonlinear, Y_nonlinear, X_nonlinear_test, Y_nonlinear_test, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Model Selection\n",
    "One of the most important pieces of machine learning is model validation: that is, checking how well your model fits a given dataset.\n",
    "Consider the digits example we've been looking at previously. How might we check how well our model fits the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "print X[21,:].reshape(8,8)\n",
    "print y[21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our simplest classification algorithm: KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "import numpy\n",
    "\n",
    "KNN_classifier = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "#KNN_classifier = ensemble.RandomForestClassifier(n_estimators=50)\n",
    "KNN_classifier.fit(X, y)\n",
    "y_pred = KNN_classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"{0:.2f}\".format(metrics.accuracy_score(y, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Sets\n",
    "Last week we discussed validation sets very briefly. It is a labeled dataset which is kept separate from training data. We use this data to test out the model on unseen data.\n",
    "Above we made the mistake of testing our data on the same set of data that was used for training. This is not generally a good idea. If we optimize our estimator this way, we will tend to **over-fit** the data: that is, we learn the noise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.3)\n",
    "X_train.shape, X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,51):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=i)\n",
    "    #knn = ensemble.RandomForestClassifier(n_estimators=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_validation)\n",
    "    print \"Validation Accuracy (neighbors = {}): {}\".format(i, metrics.accuracy_score(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold Cross Validation\n",
    "One problem with validation sets is that you \"lose\" some of the data. Above, we've only used 70% of the data for the training, and used 30% for the validation. We don't get to train with the remaining 30% of the data.  \n",
    "So to kind of normalize the result we perform k-fold cross validation. \n",
    "* Divide the labeled data into k equal parts\n",
    "* for i:=1 to k:\n",
    "* validation data = part[i]\n",
    "* train on remaining parts\n",
    "* get you prediction accuracy  \n",
    "With the above method we will have k accuracies.  **5-fold cross validation** is the most popular validation technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "for i in range(1,11):\n",
    "    cv = cross_val_score(neighbors.KNeighborsClassifier(n_neighbors=i), X, y, cv=5)\n",
    "    print \"Accuracy (neighbors = {}): {}\".format(i,cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
